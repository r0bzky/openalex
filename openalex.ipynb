{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76096f7c-a9f1-4515-9c29-d07a685e532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c27b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all functions\n",
    "\n",
    "def fetch_cited_by(url, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    params['per_page'] = 100  # items per page\n",
    "    data_frames = []\n",
    "    page = 1\n",
    "    work_id = url.split('/')[-1]  # extract work id from URL\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        while True:\n",
    "            params['page'] = page\n",
    "            response = session.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            page_results = data.get('results', [])\n",
    "            if page_results:\n",
    "                # Temporarily drop abstracts before flattening\n",
    "                abstracts = [r.pop('abstract_inverted_index', None) for r in page_results]\n",
    "                df_page = pd.json_normalize(page_results, sep='_')\n",
    "                df_page['abstract_inverted_index'] = abstracts\n",
    "                # Tag each record with the seed_id\n",
    "                df_page['seed_id'] = work_id\n",
    "                data_frames.append(df_page)\n",
    "            meta = data.get('meta', {})\n",
    "            total_pages = meta.get('page_count', 1)\n",
    "            print(f\"Fetched page {page} of {total_pages} for cited_by work {work_id}\")\n",
    "            if page >= total_pages:\n",
    "                break\n",
    "            page += 1\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True) if data_frames else pd.DataFrame()\n",
    "\n",
    "def fetch_ref(chunks, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    params['per_page'] = 50  # items per page for referenced works\n",
    "    data_frames = []\n",
    "    with requests.Session() as session:\n",
    "        for seed_id, chunk in chunks:\n",
    "            # Build the URL using the filter field \"ids.openalex\"\n",
    "            url = f\"https://api.openalex.org/works?filter=ids.openalex:{chunk}\"\n",
    "            page = 1\n",
    "            while True:\n",
    "                params['page'] = page\n",
    "                response = session.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                page_results = data.get('results', [])\n",
    "                if page_results:\n",
    "                    abstracts = [r.pop('abstract_inverted_index', None) for r in page_results]\n",
    "                    df_page = pd.json_normalize(page_results, sep='_')\n",
    "                    df_page['abstract_inverted_index'] = abstracts\n",
    "                    df_page['seed_id'] = seed_id\n",
    "                    data_frames.append(df_page)\n",
    "                meta = data.get('meta', {})\n",
    "                total_pages = meta.get('page_count', 1)\n",
    "                print(f\"Fetched page {page} of {total_pages} for chunk: {chunk}\")\n",
    "                if page >= total_pages:\n",
    "                    break\n",
    "                page += 1\n",
    "    return pd.concat(data_frames, ignore_index=True) if data_frames else pd.DataFrame()\n",
    "\n",
    "def get_seed_data(seed_id):\n",
    "    seed_url = f\"https://api.openalex.org/works/{seed_id}\"\n",
    "    response = requests.get(seed_url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def process_seed(seed_id):\n",
    "    # Retrieve seed paper data\n",
    "    seed_data = get_seed_data(seed_id)\n",
    "    \n",
    "    # Process referenced works into 50-item chunks\n",
    "    referenced = seed_data.get('referenced_works', [])\n",
    "    chunks = [\n",
    "        (seed_id, \"|\".join(referenced[i:i+50]))\n",
    "        for i in range(0, len(referenced), 50)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing referenced works for seed {seed_id}\")\n",
    "    df_ref = fetch_ref(chunks)\n",
    "    \n",
    "    # Process cited_by works using the cited_by API URL\n",
    "    cited_by_url = seed_data.get('cited_by_api_url')\n",
    "    if cited_by_url:\n",
    "        print(f\"Processing cited_by works for seed {seed_id}\")\n",
    "        df_cited = fetch_cited_by(cited_by_url)\n",
    "    else:\n",
    "        print(f\"No cited_by_api_url found for seed {seed_id}\")\n",
    "        df_cited = pd.DataFrame()\n",
    "    \n",
    "    return df_ref, df_cited\n",
    "\n",
    "def process_seeds(seed_ids):\n",
    "    master_ref_df = pd.DataFrame()\n",
    "    master_cited_df = pd.DataFrame()\n",
    "    \n",
    "    for seed_id in seed_ids:\n",
    "        print(f\"\\n=== Processing seed paper {seed_id} ===\")\n",
    "        df_ref, df_cited = process_seed(seed_id)\n",
    "        master_ref_df = pd.concat([master_ref_df, df_ref], ignore_index=True)\n",
    "        master_cited_df = pd.concat([master_cited_df, df_cited], ignore_index=True)\n",
    "    \n",
    "    # Tag the type of relationship for later chaining\n",
    "    master_ref_df['relationship'] = 'referenced'\n",
    "    master_cited_df['relationship'] = 'cited_by'\n",
    "    \n",
    "    # Combine both DataFrames\n",
    "    combined_df = pd.concat([master_ref_df, master_cited_df], ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First level: Process the original seed papers\n",
    "seed_ids = ['W85815303', 'W2001526706']  # Replace or add your seed paper IDs here\n",
    "#'W85815303' Immerwahr & Foleno (2000)\n",
    "#'W2001526706' #Doyle (2016)\n",
    "first_level_df = process_seeds(seed_ids)\n",
    "print(\"First-level results:\")\n",
    "print(first_level_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract unique work IDs from the first-level results (assuming the field 'id' holds the OpenAlex work ID)\n",
    "# You can also filter by relationship if needed.\n",
    "new_seed_ids = first_level_df['id'].dropna().unique().tolist()\n",
    "\n",
    "# Optional: Exclude original seed IDs from the second-level seeds\n",
    "second_level_seed_ids = [sid for sid in new_seed_ids if sid not in seed_ids]\n",
    "\n",
    "print(f\"Second-level seed IDs (new seeds): {second_level_seed_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26429008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second level: Use these new seed IDs for further citation chaining\n",
    "second_level_df = process_seeds(second_level_seed_ids)\n",
    "print(\"Second-level results:\")\n",
    "print(second_level_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6f7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and clean\n",
    "merged = pd.concat([first_level_df, second_level_df], ignore_index=True)\n",
    "\n",
    "merged_clean = second_level_df.query(\"is_retracted != True\") # Throw away retracted works\n",
    "merged_clean = merged_clean[merged_clean['cited_by_count'] > 0]\n",
    "\n",
    "merged_clean['id'] = merged_clean['id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "merged_clean['seed_id'] = merged_clean['seed_id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78bacda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main dataframe\n",
    "papers_cols = [\n",
    "    'id', 'title', 'primary_location_landing_page_url', 'publication_date', 'publication_year',\n",
    "    'language', 'type', 'open_access_is_oa', 'cited_by_count', 'fwci',\n",
    "    'citation_normalized_percentile_is_in_top_1_percent',\n",
    "    'citation_normalized_percentile_is_in_top_10_percent', 'relationship', 'seed_id'\n",
    "]\n",
    "\n",
    "papers = merged_clean[papers_cols]\n",
    "papers = papers.drop_duplicates()\n",
    "\n",
    "papers['citation_normalized_percentile_is_in_top_1_percent'] = papers['citation_normalized_percentile_is_in_top_1_percent'].astype(bool)\n",
    "papers['citation_normalized_percentile_is_in_top_10_percent'] = papers['citation_normalized_percentile_is_in_top_10_percent'].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ee552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authors dataframe\n",
    "authors = pd.json_normalize(\n",
    "    merged_clean.to_dict(orient='records'),\n",
    "    record_path='authorships',\n",
    "    meta=['id']\n",
    ")\n",
    "\n",
    "authors_cols = [\n",
    "    'id', 'author.id', 'author.display_name', 'author.orcid',\n",
    "    'raw_author_name', 'author_position', 'is_corresponding'\n",
    "]\n",
    "\n",
    "authors_clean = authors[authors_cols].drop_duplicates(subset=['author.id'])\n",
    "authors_clean['author.id'] = authors_clean['author.id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "authors_clean['author.orcid'] = authors_clean['author.orcid'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_records = []\n",
    "# Loop over each paper in the merged_clean dataframe\n",
    "for idx, row in merged_clean.iterrows():\n",
    "    paper_id = row['id']\n",
    "    abstract_dict = row.get('abstract_inverted_index')\n",
    "    # Make sure we have a dictionary\n",
    "    if isinstance(abstract_dict, dict):\n",
    "        # For each word in the abstract dictionary, create a new record\n",
    "        for word, positions in abstract_dict.items():\n",
    "            abstract_records.append({\n",
    "                'paper_id': paper_id,\n",
    "                'word': word,\n",
    "                'positions': positions  # this can be a list, or you can convert it to a string if needed\n",
    "            })\n",
    "\n",
    "# Create a new DataFrame from these records\n",
    "abstracts = pd.DataFrame(abstract_records)\n",
    "print(abstracts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05930649",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = authors_clean[['id', 'author.id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a051591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to (or create) the SQLite database\n",
    "conn = sqlite3.connect(\"openalex.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'papers' table\n",
    "# -------------------------------\n",
    "cursor.execute(\"DROP TABLE IF EXISTS papers;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE papers (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    primary_location_landing_page_url TEXT,\n",
    "    publication_date TEXT,\n",
    "    publication_year INTEGER,\n",
    "    language TEXT,\n",
    "    type TEXT,\n",
    "    open_access_is_oa BOOLEAN,\n",
    "    cited_by_count INTEGER,\n",
    "    fwci REAL,\n",
    "    citation_normalized_percentile_is_in_top_1_percent BOOLEAN,\n",
    "    citation_normalized_percentile_is_in_top_10_percent BOOLEAN\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'authors' table\n",
    "# -------------------------------\n",
    "# Here, 'id' represents the paper_id from the merged_clean DataFrame.\n",
    "# We'll rename \"author.id\" to \"author_id\" and \"author.display_name\" to \"author_display_name\"\n",
    "cursor.execute(\"DROP TABLE IF EXISTS authors;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE authors (\n",
    "    paper_id TEXT,\n",
    "    author_id TEXT,\n",
    "    author_display_name TEXT,\n",
    "    author_orcid TEXT,\n",
    "    raw_author_name TEXT,\n",
    "    author_position TEXT,\n",
    "    is_corresponding BOOLEAN,\n",
    "    PRIMARY KEY (author_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'abstracts' table\n",
    "# -------------------------------\n",
    "cursor.execute(\"DROP TABLE IF EXISTS abstracts;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE abstracts (\n",
    "    paper_id TEXT,\n",
    "    word TEXT,\n",
    "    positions TEXT,\n",
    "    record_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'bridge' table\n",
    "# -------------------------------\n",
    "# This table serves as a many-to-many join between papers and authors.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS bridge;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE bridge (\n",
    "    paper_id TEXT,\n",
    "    author_id TEXT,\n",
    "    PRIMARY KEY (author_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "    FOREIGN KEY (author_id) REFERENCES authors(author_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Now, insert data from your DataFrames.\n",
    "# Make sure to adjust column names in your DataFrames if necessary.\n",
    "# For example, if your authors_clean DataFrame still has columns \"author.id\", rename them:\n",
    "authors_clean = authors_clean.rename(columns={\n",
    "    'id': 'paper_id',\n",
    "    'author.id': 'author_id',\n",
    "    'author.display_name': 'author_display_name',\n",
    "    'author.orcid': 'author_orcid'\n",
    "})\n",
    "\n",
    "bridge = bridge.rename(columns={\n",
    "    'id': 'paper_id',\n",
    "    'author.id': 'author_id',\n",
    "})\n",
    "\n",
    "abstracts['positions'] = abstracts['positions'].apply(\n",
    "    lambda x: \",\".join(map(str, x)) if isinstance(x, list) else str(x)\n",
    ")\n",
    "\n",
    "# Insert the papers data (from merged_clean)\n",
    "papers.to_sql(\"papers\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the authors data (authors_clean)\n",
    "authors_clean.to_sql(\"authors\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the abstracts data (abstracts_df)\n",
    "abstracts.to_sql(\"abstracts\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the bridge table (bridge_table)\n",
    "bridge.to_sql(\"bridge\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b73a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.info()\n",
    "authors_clean.info()\n",
    "bridge.info()\n",
    "abstracts.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
