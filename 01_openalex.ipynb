{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76096f7c-a9f1-4515-9c29-d07a685e532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7a8c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    # Create a custom session with retry logic and polite pool configuration\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Configure retry strategy (5 retries with exponential backoff)\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    \n",
    "    # Mount custom adapter with retry logic\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    \n",
    "    # Set polite pool headers with your institutional email\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'UTAustinResearch/1.0 (mailto:robert.stein@utexas.edu)',\n",
    "        'From': 'robert.stein@utexas.edu'\n",
    "    })\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c27b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all functions\n",
    "def fetch_cited_by(url, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    params['per_page'] = 100  # items per page\n",
    "    data_frames = []\n",
    "    page = 1\n",
    "    work_id = url.split('/')[-1]  # extract work id from URL\n",
    "\n",
    "    with create_session() as session:\n",
    "        while True:\n",
    "            params['page'] = page\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            response = session.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            page_results = data.get('results', [])\n",
    "            if page_results:\n",
    "                # Temporarily drop abstracts before flattening\n",
    "                abstracts = [r.pop('abstract_inverted_index', None) for r in page_results]\n",
    "                df_page = pd.json_normalize(page_results, sep='_')\n",
    "                df_page['abstract_inverted_index'] = abstracts\n",
    "                # Tag each record with the seed_id\n",
    "                df_page['seed_id'] = work_id\n",
    "                data_frames.append(df_page)\n",
    "            meta = data.get('meta', {})\n",
    "            total_pages = meta.get('page_count', 1)\n",
    "            print(f\"Fetched page {page} of {total_pages} for cited_by work {work_id}\")\n",
    "            if page >= total_pages:\n",
    "                break\n",
    "            page += 1\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True) if data_frames else pd.DataFrame()\n",
    "\n",
    "def fetch_ref(chunks, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    params['per_page'] = 50  # items per page for referenced works\n",
    "    data_frames = []\n",
    "    with create_session() as session:\n",
    "        for seed_id, chunk in chunks:\n",
    "            # Build the URL using the filter field \"ids.openalex\"\n",
    "            url = f\"https://api.openalex.org/works?filter=ids.openalex:{chunk}\"\n",
    "            page = 1\n",
    "            while True:\n",
    "                time.sleep(1)\n",
    "                params['page'] = page\n",
    "                response = session.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                page_results = data.get('results', [])\n",
    "                if page_results:\n",
    "                    abstracts = [r.pop('abstract_inverted_index', None) for r in page_results]\n",
    "                    df_page = pd.json_normalize(page_results, sep='_')\n",
    "                    df_page['abstract_inverted_index'] = abstracts\n",
    "                    df_page['seed_id'] = seed_id\n",
    "                    data_frames.append(df_page)\n",
    "                meta = data.get('meta', {})\n",
    "                total_pages = meta.get('page_count', 1)\n",
    "                print(f\"Fetched page {page} of {total_pages} for chunk: {chunk}\")\n",
    "                if page >= total_pages:\n",
    "                    break\n",
    "                page += 1\n",
    "    return pd.concat(data_frames, ignore_index=True) if data_frames else pd.DataFrame()\n",
    "\n",
    "def get_seed_data(seed_id):\n",
    "    seed_url = f\"https://api.openalex.org/works/{seed_id}\"\n",
    "    response = requests.get(seed_url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def process_seed(seed_id):\n",
    "    # Retrieve seed paper data\n",
    "    seed_data = get_seed_data(seed_id)\n",
    "    \n",
    "    # Process referenced works into 50-item chunks\n",
    "    referenced = seed_data.get('referenced_works', [])\n",
    "    chunks = [\n",
    "        (seed_id, \"|\".join(referenced[i:i+50]))\n",
    "        for i in range(0, len(referenced), 50)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing referenced works for seed {seed_id}\")\n",
    "    df_ref = fetch_ref(chunks)\n",
    "    \n",
    "    # Process cited_by works using the cited_by API URL\n",
    "    cited_by_url = seed_data.get('cited_by_api_url')\n",
    "    if cited_by_url:\n",
    "        print(f\"Processing cited_by works for seed {seed_id}\")\n",
    "        df_cited = fetch_cited_by(cited_by_url)\n",
    "    else:\n",
    "        print(f\"No cited_by_api_url found for seed {seed_id}\")\n",
    "        df_cited = pd.DataFrame()\n",
    "    \n",
    "    return df_ref, df_cited\n",
    "\n",
    "def process_seeds(seed_ids):\n",
    "    master_ref_df = pd.DataFrame()\n",
    "    master_cited_df = pd.DataFrame()\n",
    "    \n",
    "    for seed_id in seed_ids:\n",
    "        print(f\"\\n=== Processing seed paper {seed_id} ===\")\n",
    "        df_ref, df_cited = process_seed(seed_id)\n",
    "        master_ref_df = pd.concat([master_ref_df, df_ref], ignore_index=True)\n",
    "        master_cited_df = pd.concat([master_cited_df, df_cited], ignore_index=True)\n",
    "    \n",
    "    # Tag the type of relationship for later chaining\n",
    "    master_ref_df['relationship'] = 'referenced'\n",
    "    master_cited_df['relationship'] = 'cited_by'\n",
    "    \n",
    "    # Combine both DataFrames\n",
    "    combined_df = pd.concat([master_ref_df, master_cited_df], ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a80c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_text(inverted_index):\n",
    "    if not isinstance(inverted_index, dict) or not inverted_index:\n",
    "        return pd.NA  # Returns missing value for non-dict/empty inputs\n",
    "    \n",
    "    try:\n",
    "        all_positions = [pos for positions in inverted_index.values() for pos in positions]\n",
    "        if not all_positions:\n",
    "            return pd.NA\n",
    "            \n",
    "        max_index = max(all_positions)\n",
    "        tokens = [None] * (max_index + 1)\n",
    "        \n",
    "        for token, positions in inverted_index.items():\n",
    "            for pos in positions:\n",
    "                if pos <= max_index:\n",
    "                    tokens[pos] = token\n",
    "                    \n",
    "        # Join only valid tokens and filter empty results\n",
    "        reconstructed = ' '.join(filter(None, tokens))\n",
    "        return reconstructed if reconstructed else pd.NA\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reconstructing abstract: {str(e)}\")\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First level: Process the original seed papers\n",
    "seed_ids = ['W2001526706']  # Replace or add your seed paper IDs here\n",
    "#'W85815303' Immerwahr & Foleno (2000)\n",
    "#'W2001526706' #Doyle (2016)\n",
    "first_level_df = process_seeds(seed_ids)\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract unique work IDs from the first-level results (assuming the field 'id' holds the OpenAlex work ID)\n",
    "# You can also filter by relationship if needed.\n",
    "new_seed_ids = first_level_df['id'].dropna().unique().tolist()\n",
    "\n",
    "# Optional: Exclude original seed IDs from the second-level seeds\n",
    "second_level_seed_ids = [sid for sid in new_seed_ids if sid not in seed_ids]\n",
    "\n",
    "print(f\"Second-level seed IDs (new seeds): {second_level_seed_ids}\")\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26429008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second level: Use these new seed IDs for further citation chaining\n",
    "second_level_df = process_seeds(second_level_seed_ids)\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and clean\n",
    "merged = pd.concat([first_level_df, second_level_df], ignore_index=True)\n",
    "\n",
    "merged_clean = second_level_df.query(\"is_retracted != True\") # Throw away retracted works\n",
    "merged_clean = merged_clean[merged_clean['cited_by_count'] > 0]\n",
    "\n",
    "merged_clean['id'] = merged_clean['id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "merged_clean['seed_id'] = merged_clean['seed_id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "\n",
    "# Clean citation percentile, from str to boolean\n",
    "merged_clean['citation_normalized_percentile_is_in_top_1_percent'] = merged_clean['citation_normalized_percentile_is_in_top_1_percent'].astype(bool)\n",
    "merged_clean['citation_normalized_percentile_is_in_top_10_percent'] = merged_clean['citation_normalized_percentile_is_in_top_10_percent'].astype(bool)\n",
    "\n",
    "# Reconstruct abstracts from inverted indicies\n",
    "merged_clean['abstract'] = merged_clean['abstract_inverted_index'].apply(reconstruct_text)\n",
    "\n",
    "# Drop all empty cols\n",
    "merged_clean = merged_clean.dropna(axis='columns', how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78bacda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main dataframe\n",
    "papers_cols = [\n",
    "    'id', 'title', 'primary_location_landing_page_url', 'publication_date', 'publication_year',\n",
    "    'language', 'type', 'open_access_is_oa', 'cited_by_count', 'fwci',\n",
    "    'citation_normalized_percentile_is_in_top_1_percent',\n",
    "    'citation_normalized_percentile_is_in_top_10_percent', 'has_fulltext', 'fulltext_origin'\n",
    "]\n",
    "\n",
    "papers = merged_clean[papers_cols]\n",
    "\n",
    "# Columns to consider for duplicate checking\n",
    "cols_to_check = papers.columns.difference(['cited_by_count']).tolist()\n",
    "\n",
    "# Remove duplicates based on all columns EXCEPT cited_by_count\n",
    "papers = papers.drop_duplicates(\n",
    "    subset=cols_to_check,  # All columns except cited_by_count\n",
    "    keep='first'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72ee552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authors dataframe\n",
    "authors = pd.json_normalize(\n",
    "    merged_clean.to_dict(orient='records'),\n",
    "    record_path='authorships',\n",
    "    meta=['id']\n",
    ")\n",
    "\n",
    "authors_cols = [\n",
    "    'id', 'author.id', 'author.display_name', 'author.orcid',\n",
    "    'raw_author_name', 'author_position', 'is_corresponding'\n",
    "]\n",
    "\n",
    "authors_clean = authors[authors_cols].drop_duplicates(subset=['author.id'])\n",
    "authors_clean['author.id'] = authors_clean['author.id'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "authors_clean['author.orcid'] = authors_clean['author.orcid'].apply(lambda x: x.rsplit('/', 1)[-1] if isinstance(x, str) else x)\n",
    "\n",
    "authors_clean = authors_clean.rename(columns={\n",
    "    'id': 'paper_id',\n",
    "    'author.id': 'author_id',\n",
    "    'author.display_name': 'author_display_name',\n",
    "    'author.orcid': 'author_orcid'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb2e69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create abstracts dataframe with un-inverted entries\n",
    "abstracts = merged_clean[['id', 'abstract']].rename(columns={\n",
    "    'id': 'paper_id'\n",
    "})\n",
    "abstracts = abstracts.dropna(subset=['abstract']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05930649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bridge dataframe, linking authors with works\n",
    "bridge = authors_clean[['paper_id', 'author_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a051591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to (or create) the SQLite database\n",
    "conn = sqlite3.connect(\"openalex.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'papers' table\n",
    "# -------------------------------\n",
    "cursor.execute(\"DROP TABLE IF EXISTS papers;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE papers (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    primary_location_landing_page_url TEXT,\n",
    "    publication_date TEXT,\n",
    "    publication_year INTEGER,\n",
    "    language TEXT,\n",
    "    type TEXT,\n",
    "    open_access_is_oa BOOLEAN,\n",
    "    cited_by_count INTEGER,\n",
    "    fwci REAL,\n",
    "    has_fulltext BOOLEAN,\n",
    "    fulltext_origin TEXT,\n",
    "    citation_normalized_percentile_is_in_top_1_percent BOOLEAN,\n",
    "    citation_normalized_percentile_is_in_top_10_percent BOOLEAN\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'authors' table\n",
    "# -------------------------------\n",
    "# Here, 'id' represents the paper_id from the merged_clean DataFrame.\n",
    "# We'll rename \"author.id\" to \"author_id\" and \"author.display_name\" to \"author_display_name\"\n",
    "cursor.execute(\"DROP TABLE IF EXISTS authors;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE authors (\n",
    "    paper_id TEXT,\n",
    "    author_id TEXT,\n",
    "    author_display_name TEXT,\n",
    "    author_orcid TEXT,\n",
    "    raw_author_name TEXT,\n",
    "    author_position TEXT,\n",
    "    is_corresponding BOOLEAN,\n",
    "    PRIMARY KEY (author_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'abstracts' table\n",
    "# -------------------------------\n",
    "cursor.execute(\"DROP TABLE IF EXISTS abstracts;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE abstracts (\n",
    "    paper_id TEXT PRIMARY KEY,\n",
    "    abstract TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# Create the 'bridge' table\n",
    "# -------------------------------\n",
    "# This table serves as a many-to-many join between papers and authors.\n",
    "cursor.execute(\"DROP TABLE IF EXISTS bridge;\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE bridge (\n",
    "    paper_id TEXT,\n",
    "    author_id TEXT,\n",
    "    PRIMARY KEY (author_id),\n",
    "    FOREIGN KEY (paper_id) REFERENCES papers(id),\n",
    "    FOREIGN KEY (author_id) REFERENCES authors(author_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Insert the papers data\n",
    "papers.to_sql(\"papers\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the authors data\n",
    "authors_clean.to_sql(\"authors\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the abstracts data\n",
    "abstracts.to_sql(\"abstracts\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Insert the bridge table\n",
    "bridge.to_sql(\"bridge\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
